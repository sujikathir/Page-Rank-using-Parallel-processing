{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unknown project with random code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9/HjnSUuLF0esxfTWlAuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujikathir/Page-Rank-using-Parallel-processing/blob/main/Page%20Rank%20using%20Map%20Reduce%20and%20Iterative%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwxEnSaGYxyK"
      },
      "source": [
        "#map_reduce.py\r\n",
        "# Defines a single function, map_reduce, which takes an input\r\n",
        "# dictionary i and applies the user-defined function mapper to each\r\n",
        "# (input_key,input_value) pair, producing a list of intermediate \r\n",
        "# keys and intermediate values.  Repeated intermediate keys then \r\n",
        "# have their values grouped into a list, and the user-defined \r\n",
        "# function reducer is applied to the intermediate key and list of \r\n",
        "# intermediate values.  The results are returned as a list.\r\n",
        "\r\n",
        "import itertools\r\n",
        "\r\n",
        "def map_reduce(i,mapper,reducer):\r\n",
        "  intermediate = []\r\n",
        "  for (key,value) in i.items():\r\n",
        "    intermediate.extend(mapper(key,value))\r\n",
        "  groups = {}\r\n",
        "  for key, group in itertools.groupby(sorted(intermediate), \r\n",
        "                                      lambda x: x[0]):\r\n",
        "    groups[key] = list([y for x, y in group])\r\n",
        "  return [reducer(intermediate_key,groups[intermediate_key])\r\n",
        "          for intermediate_key in groups] "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdC3LH8Hia2j",
        "outputId": "4a6d9f95-8ede-4243-8f65-9e397c3f4384"
      },
      "source": [
        "#pagerank_mr.py\r\n",
        "#\r\n",
        "# Computes PageRank, using a simple MapReduce library.\r\n",
        "#\r\n",
        "# MapReduce is used in two separate ways: (1) to compute\r\n",
        "# the inner product between the vector of dangling pages\r\n",
        "# (i.e., pages with no outbound links) and the current\r\n",
        "# estimated PageRank vector; and (2) to actually carry\r\n",
        "# out the update of the estimated PageRank vector.\r\n",
        "#\r\n",
        "# For a web of one million webpages the program consumes\r\n",
        "# about one gig of RAM, and takes an hour or so to run,\r\n",
        "# on a (slow) laptop with 3 gig of RAM, running Vista and\r\n",
        "# Python 2.5.\r\n",
        "\r\n",
        "#import map_reduce\r\n",
        "import numpy.random\r\n",
        "import random\r\n",
        "\r\n",
        "def paretosample(n,power=2.0):\r\n",
        "  # Returns a sample from a truncated Pareto distribution\r\n",
        "  # with probability mass function p(l) proportional to\r\n",
        "  # 1/l^power.  The distribution is truncated at l = n.\r\n",
        "\r\n",
        "  m = n+1\r\n",
        "  while m > n: m = numpy.random.zipf(power)\r\n",
        "  return m\r\n",
        "\r\n",
        "def initialize(n,power):\r\n",
        "  # Returns a Python dictionary representing a web\r\n",
        "  # with n pages, and where each page k is linked to by\r\n",
        "  # L_k random other pages.  The L_k are independent and\r\n",
        "  # identically distributed random variables with a\r\n",
        "  # shifted and truncated Pareto probability mass function\r\n",
        "  # p(l) proportional to 1/(l+1)^power.\r\n",
        "\r\n",
        "  # The representation used is a Python dictionary with\r\n",
        "  # keys 0 through n-1 representing the different pages.\r\n",
        "  # i[j][0] is the estimated PageRank, initially set at 1/n,\r\n",
        "  # i[j][1] the number of outlinks, and i[j][2] a list of\r\n",
        "  # the outlinks.\r\n",
        "\r\n",
        "  # This dictionary is used to supply (key,value) pairs to\r\n",
        "  # both mapper tasks defined below.\r\n",
        "\r\n",
        "  # initialize the dictionary\r\n",
        "  i = {} \r\n",
        "  for j in range(n): i[j] = [1.0/n,0,[]]\r\n",
        "  \r\n",
        "  # For each page, generate inlinks according to the Pareto\r\n",
        "  # distribution. Note that this is somewhat tedious, because\r\n",
        "  # the Pareto distribution governs inlinks, NOT outlinks,\r\n",
        "  # which is what our representation is adapted to represent.\r\n",
        "  # A smarter representation would give easy\r\n",
        "  # access to both, while remaining memory efficient.\r\n",
        "  for k in range(n):\r\n",
        "    lk = paretosample(n+1,power)-1\r\n",
        "    values = random.sample(range(n),lk)\r\n",
        "    for j in values:\r\n",
        "      i[j][1] += 1 # increment the outlink count for page j\r\n",
        "      i[j][2].append(k) # insert the link from j to k\r\n",
        "  return i\r\n",
        "\r\n",
        "def ip_mapper(input_key,input_value):\r\n",
        "  # The mapper used to compute the inner product between\r\n",
        "  # the vector of dangling pages and the current estimated\r\n",
        "  # PageRank.  The input is a key describing a webpage, and\r\n",
        "  # the corresponding data, including the estimated pagerank.\r\n",
        "  # The mapper returns [(1,pagerank)] if the page is dangling,\r\n",
        "  # and otherwise returns nothing.\r\n",
        "  \r\n",
        "  if input_value[1] == 0: return [(1,input_value[0])]\r\n",
        "  else: return []\r\n",
        "\r\n",
        "def ip_reducer(input_key,input_value_list):\r\n",
        "  # The reducer used to compute the inner product.  Simply\r\n",
        "  # sums the pageranks listed in the input value list, which\r\n",
        "  # are all the pageranks for dangling pages.\r\n",
        "\r\n",
        "  return sum(input_value_list)\r\n",
        "\r\n",
        "def pr_mapper(input_key,input_value):\r\n",
        "  # The mapper used to update the PageRank estimate.  Takes\r\n",
        "  # as input a key for a webpage, and as a value the corresponding\r\n",
        "  # data, as described in the function initialize.  It returns a\r\n",
        "  # list with all outlinked pages as keys, and corresponding values\r\n",
        "  # just the PageRank of the origin page, divided by the total\r\n",
        "  # number of outlinks from the origin page.  Also appended to\r\n",
        "  # that list is a pair with key the origin page, and value 0.\r\n",
        "  # This is done to ensure that every single page ends up with at\r\n",
        "  # least one corresponding (intermediate_key,intermediate_value)\r\n",
        "  # pair output from a mapper.\r\n",
        "  \r\n",
        "  return [(input_key,0.0)]+[(outlink,input_value[0]/input_value[1])\r\n",
        "          for outlink in input_value[2]]\r\n",
        "\r\n",
        "def pr_reducer_inter(intermediate_key,intermediate_value_list,\r\n",
        "                     s,ip,n):\r\n",
        "  # This is a helper function used to define the reducer used\r\n",
        "  # to update the PageRank estimate.  Note that the helper differs\r\n",
        "  # from a standard reducer in having some additional inputs:\r\n",
        "  # s (the PageRank parameter), ip (the value of the inner product\r\n",
        "  # between the dangling pages vector and the estimated PageRank),\r\n",
        "  # and n, the number of pages.  Other than that the code is\r\n",
        "  # self-explanatory.\r\n",
        "  \r\n",
        "  return (intermediate_key,\r\n",
        "          s*sum(intermediate_value_list)+s*ip/n+(1.0-s)/n)\r\n",
        "\r\n",
        "def pagerank(i,s=0.85,tolerance=0.00001):\r\n",
        "  # Returns the PageRank vector for the web described by i,\r\n",
        "  # using parameter s.  The criterion for convergence is that\r\n",
        "  # we stop when M^(j+1)P-M^jP has length less than tolerance,\r\n",
        "  # in l1 norm.\r\n",
        "  \r\n",
        "  n = len(i)\r\n",
        "  iteration = 1\r\n",
        "  change = 2 # initial estimate of error\r\n",
        "  while change > tolerance:\r\n",
        "    print(\"Iteration: \"+str(iteration))\r\n",
        "    # Run the MapReduce job used to compute the inner product\r\n",
        "    # between the vector of dangling pages and the estimated\r\n",
        "    # PageRank.\r\n",
        "    ip_list = map_reduce(i,ip_mapper,ip_reducer)\r\n",
        "\r\n",
        "    # the if-else clause is needed in case there are no dangling\r\n",
        "    # pages, in which case MapReduce returns ip_list as the empty\r\n",
        "    # list.  Otherwise, set ip equal to the first (and only)\r\n",
        "    # member of the list returned by MapReduce.\r\n",
        "    if ip_list == []: ip = 0\r\n",
        "    else: ip = ip_list[0]\r\n",
        "\r\n",
        "    # Dynamically define the reducer used to update the PageRank\r\n",
        "    # vector, using the current values for s, ip, and n.\r\n",
        "    pr_reducer = lambda x,y: pr_reducer_inter(x,y,s,ip,n)\r\n",
        "\r\n",
        "    # Run the MapReduce job used to update the PageRank vector.\r\n",
        "    new_i = map_reduce(i,pr_mapper,pr_reducer)\r\n",
        "\r\n",
        "    # Compute the new estimate of error.\r\n",
        "    change = sum([abs(new_i[j][1]-i[j][0]) for j in range(n)])\r\n",
        "    print(\"Change in l1 norm: \"+str(change))\r\n",
        "\r\n",
        "    # Update the estimate PageRank vector.\r\n",
        "    for j in range(n): i[j][0] = new_i[j][1]\r\n",
        "    iteration += 1\r\n",
        "  return i\r\n",
        "\r\n",
        "n = 1000 # works up to about 1000000 pages\r\n",
        "i = initialize(n,2.0)\r\n",
        "new_i = pagerank(i,0.85,0.0001)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1\n",
            "Change in l1 norm: 1.1734960761904791\n",
            "Iteration: 2\n",
            "Change in l1 norm: 0.5373847834016119\n",
            "Iteration: 3\n",
            "Change in l1 norm: 0.27438467564810015\n",
            "Iteration: 4\n",
            "Change in l1 norm: 0.1637817352221548\n",
            "Iteration: 5\n",
            "Change in l1 norm: 0.0897647588863386\n",
            "Iteration: 6\n",
            "Change in l1 norm: 0.05322774757919292\n",
            "Iteration: 7\n",
            "Change in l1 norm: 0.03256884058288466\n",
            "Iteration: 8\n",
            "Change in l1 norm: 0.019112469155368753\n",
            "Iteration: 9\n",
            "Change in l1 norm: 0.010268517960149883\n",
            "Iteration: 10\n",
            "Change in l1 norm: 0.005840204250583992\n",
            "Iteration: 11\n",
            "Change in l1 norm: 0.0034129330923384346\n",
            "Iteration: 12\n",
            "Change in l1 norm: 0.0018961608977573763\n",
            "Iteration: 13\n",
            "Change in l1 norm: 0.0010526496331738125\n",
            "Iteration: 14\n",
            "Change in l1 norm: 0.0006706209179380719\n",
            "Iteration: 15\n",
            "Change in l1 norm: 0.00039262350175472656\n",
            "Iteration: 16\n",
            "Change in l1 norm: 0.0002255210265589825\n",
            "Iteration: 17\n",
            "Change in l1 norm: 0.00012881402763106447\n",
            "Iteration: 18\n",
            "Change in l1 norm: 7.547542472012173e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uyAkka_igs_"
      },
      "source": [
        "def pagerank(G, alpha=0.85, personalization=None, \r\n",
        "\t\t\tmax_iter=100, tol=1.0e-6, nstart=None, weight='weight', \r\n",
        "\t\t\tdangling=None): \r\n",
        "\t\"\"\"Return the PageRank of the nodes in the graph. \r\n",
        "\r\n",
        "\tPageRank computes a ranking of the nodes in the graph G based on \r\n",
        "\tthe structure of the incoming links. It was originally designed as \r\n",
        "\tan algorithm to rank web pages. \r\n",
        "\r\n",
        "\tParameters \r\n",
        "\t---------- \r\n",
        "\tG : graph \r\n",
        "\tA NetworkX graph. Undirected graphs will be converted to a directed \r\n",
        "\tgraph with two directed edges for each undirected edge. \r\n",
        "\r\n",
        "\talpha : float, optional \r\n",
        "\tDamping parameter for PageRank, default=0.85. \r\n",
        "\r\n",
        "\tpersonalization: dict, optional \r\n",
        "\tThe \"personalization vector\" consisting of a dictionary with a \r\n",
        "\tkey for every graph node and nonzero personalization value for each node. \r\n",
        "\tBy default, a uniform distribution is used. \r\n",
        "\r\n",
        "\tmax_iter : integer, optional \r\n",
        "\tMaximum number of iterations in power method eigenvalue solver. \r\n",
        "\r\n",
        "\ttol : float, optional \r\n",
        "\tError tolerance used to check convergence in power method solver. \r\n",
        "\r\n",
        "\tnstart : dictionary, optional \r\n",
        "\tStarting value of PageRank iteration for each node. \r\n",
        "\r\n",
        "\tweight : key, optional \r\n",
        "\tEdge data key to use as weight. If None weights are set to 1. \r\n",
        "\r\n",
        "\tdangling: dict, optional \r\n",
        "\tThe outedges to be assigned to any \"dangling\" nodes, i.e., nodes without \r\n",
        "\tany outedges. The dict key is the node the outedge points to and the dict \r\n",
        "\tvalue is the weight of that outedge. By default, dangling nodes are given \r\n",
        "\toutedges according to the personalization vector (uniform if not \r\n",
        "\tspecified). This must be selected to result in an irreducible transition \r\n",
        "\tmatrix (see notes under google_matrix). It may be common to have the \r\n",
        "\tdangling dict to be the same as the personalization dict. \r\n",
        "\r\n",
        "\tReturns \r\n",
        "\t------- \r\n",
        "\tpagerank : dictionary \r\n",
        "\tDictionary of nodes with PageRank as value \r\n",
        "\r\n",
        "\tNotes \r\n",
        "\t----- \r\n",
        "\tThe eigenvector calculation is done by the power iteration method \r\n",
        "\tand has no guarantee of convergence. The iteration will stop \r\n",
        "\tafter max_iter iterations or an error tolerance of \r\n",
        "\tnumber_of_nodes(G)*tol has been reached. \r\n",
        "\r\n",
        "\tThe PageRank algorithm was designed for directed graphs but this \r\n",
        "\talgorithm does not check if the input graph is directed and will \r\n",
        "\texecute on undirected graphs by converting each edge in the \r\n",
        "\tdirected graph to two edges. \r\n",
        "\r\n",
        "\t\r\n",
        "\t\"\"\"\r\n",
        "\tif len(G) == 0: \r\n",
        "\t\treturn {} \r\n",
        "\r\n",
        "\tif not G.is_directed(): \r\n",
        "\t\tD = G.to_directed() \r\n",
        "\telse: \r\n",
        "\t\tD = G \r\n",
        "\r\n",
        "\t# Create a copy in (right) stochastic form \r\n",
        "\tW = nx.stochastic_graph(D, weight=weight) \r\n",
        "\tN = W.number_of_nodes() \r\n",
        "\r\n",
        "\t# Choose fixed starting vector if not given \r\n",
        "\tif nstart is None: \r\n",
        "\t\tx = dict.fromkeys(W, 1.0 / N) \r\n",
        "\telse: \r\n",
        "\t\t# Normalized nstart vector \r\n",
        "\t\ts = float(sum(nstart.values())) \r\n",
        "\t\tx = dict((k, v / s) for k, v in nstart.items()) \r\n",
        "\r\n",
        "\tif personalization is None: \r\n",
        "\r\n",
        "\t\t# Assign uniform personalization vector if not given \r\n",
        "\t\tp = dict.fromkeys(W, 1.0 / N) \r\n",
        "\telse: \r\n",
        "\t\tmissing = set(G) - set(personalization) \r\n",
        "\t\tif missing: \r\n",
        "\t\t\traise NetworkXError('Personalization dictionary '\r\n",
        "\t\t\t\t\t\t\t\t'must have a value for every node. '\r\n",
        "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing) \r\n",
        "\t\ts = float(sum(personalization.values())) \r\n",
        "\t\tp = dict((k, v / s) for k, v in personalization.items()) \r\n",
        "\r\n",
        "\tif dangling is None: \r\n",
        "\r\n",
        "\t\t# Use personalization vector if dangling vector not specified \r\n",
        "\t\tdangling_weights = p \r\n",
        "\telse: \r\n",
        "\t\tmissing = set(G) - set(dangling) \r\n",
        "\t\tif missing: \r\n",
        "\t\t\traise NetworkXError('Dangling node dictionary '\r\n",
        "\t\t\t\t\t\t\t\t'must have a value for every node. '\r\n",
        "\t\t\t\t\t\t\t\t'Missing nodes %s' % missing) \r\n",
        "\t\ts = float(sum(dangling.values())) \r\n",
        "\t\tdangling_weights = dict((k, v/s) for k, v in dangling.items()) \r\n",
        "\tdangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0] \r\n",
        "\r\n",
        "\t# power iteration: make up to max_iter iterations \r\n",
        "\tfor _ in range(max_iter): \r\n",
        "\t\txlast = x \r\n",
        "\t\tx = dict.fromkeys(xlast.keys(), 0) \r\n",
        "\t\tdanglesum = alpha * sum(xlast[n] for n in dangling_nodes) \r\n",
        "\t\tfor n in x: \r\n",
        "\r\n",
        "\t\t\t# this matrix multiply looks odd because it is \r\n",
        "\t\t\t# doing a left multiply x^T=xlast^T*W \r\n",
        "\t\t\tfor nbr in W[n]: \r\n",
        "\t\t\t\tx[nbr] += alpha * xlast[n] * W[n][nbr][weight] \r\n",
        "\t\t\tx[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n] \r\n",
        "\r\n",
        "\t\t# check convergence, l1 norm \r\n",
        "\t\terr = sum([abs(x[n] - xlast[n]) for n in x]) \r\n",
        "\t\tif err < N*tol: \r\n",
        "\t\t\treturn x \r\n",
        "\traise NetworkXError('pagerank: power iteration failed to converge '\r\n",
        "\t\t\t\t\t\t'in %d iterations.' % max_iter) \r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh1kn6I3m5kv",
        "outputId": "e446089c-5801-4b9c-e3fd-5b6b51236713"
      },
      "source": [
        ">>> import networkx as nx \r\n",
        ">>> G=nx.barabasi_albert_graph(60,41) \r\n",
        ">>> pr=nx.pagerank(G,0.4) \r\n",
        ">>> pr \r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.012762116280188423,\n",
              " 1: 0.012562687224119184,\n",
              " 2: 0.0129711710223643,\n",
              " 3: 0.013365134775054436,\n",
              " 4: 0.013779898628840296,\n",
              " 5: 0.012971389152932836,\n",
              " 6: 0.013157950185175158,\n",
              " 7: 0.012772899264751591,\n",
              " 8: 0.012380359557936467,\n",
              " 9: 0.012759639719288408,\n",
              " 10: 0.012588768676461342,\n",
              " 11: 0.013384431289396613,\n",
              " 12: 0.01296107858897791,\n",
              " 13: 0.013159473196795958,\n",
              " 14: 0.012769225546855747,\n",
              " 15: 0.012362901900532306,\n",
              " 16: 0.012985477629885585,\n",
              " 17: 0.013565853212868122,\n",
              " 18: 0.013576473560975456,\n",
              " 19: 0.013570168680337925,\n",
              " 20: 0.013168042539239382,\n",
              " 21: 0.013378300481784863,\n",
              " 22: 0.012757855624873271,\n",
              " 23: 0.0127693662085754,\n",
              " 24: 0.01216664954385023,\n",
              " 25: 0.01358142778182973,\n",
              " 26: 0.012375013757911441,\n",
              " 27: 0.013170979395513673,\n",
              " 28: 0.01315882900411266,\n",
              " 29: 0.012970384573172017,\n",
              " 30: 0.013371774072519878,\n",
              " 31: 0.01215716949222956,\n",
              " 32: 0.012978271748507937,\n",
              " 33: 0.012784361399091915,\n",
              " 34: 0.013174837466978983,\n",
              " 35: 0.012575164355548596,\n",
              " 36: 0.012963473471386579,\n",
              " 37: 0.01257724964720176,\n",
              " 38: 0.012752503830578662,\n",
              " 39: 0.01234271559430143,\n",
              " 40: 0.013575199140384719,\n",
              " 41: 0.028041552268646703,\n",
              " 42: 0.027694399428697072,\n",
              " 43: 0.027021580564833485,\n",
              " 44: 0.026828474066118362,\n",
              " 45: 0.0265032578086433,\n",
              " 46: 0.025918740559451717,\n",
              " 47: 0.025755160517739864,\n",
              " 48: 0.025332292818403747,\n",
              " 49: 0.025266212397051242,\n",
              " 50: 0.02466097333642664,\n",
              " 51: 0.024311910000790252,\n",
              " 52: 0.02390673182698568,\n",
              " 53: 0.02358373296734838,\n",
              " 54: 0.023393237130492173,\n",
              " 55: 0.022516313986633205,\n",
              " 56: 0.022146071801871253,\n",
              " 57: 0.022002589783565635,\n",
              " 58: 0.022021090481591135,\n",
              " 59: 0.021939011031379484}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8LBBSQ9m9Va"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}